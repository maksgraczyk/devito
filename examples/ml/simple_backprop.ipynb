{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple backpropagation in Devito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll implement a simple convolutional neural network (CNN) in Devito, run a forward pass through it and then use backpropagation to obtain gradients necessary for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN will have the following structure:\n",
    "1. Max pool layer: input size 1x2x4x4, kernel size 2x2, stride 1x1\n",
    "2. Convolutional layer: input size 1x2x3x3, kernel size 2x2x2, stride 1x1, activation ReLU\n",
    "3. Flattening layer\n",
    "4. Fully connected layer: input size 8x1, kernel size 3x8, activation softmax\n",
    "\n",
    "*Size glossary: batch size x channels x height x width **or** output channels x height x width **or** height x width. Height and width are equivalent to rows and columns.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All parameters for the forward pass will be (pseudo)random numbers generated by `np.random.rand`. Therefore, different results will be obtained each time the notebook is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import devito.ml\n",
    "import numpy as np\n",
    "from sympy.functions import sign\n",
    "from devito import Operator, Eq, Inc\n",
    "from devito.ml.activation import ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(conv_kernel, conv_bias, fc_weights, fc_bias, input_data, expected_results):\n",
    "    layer1 = devito.ml.MaxPooling(kernel_size=(2, 2),\n",
    "                                  input_size=(1, 2, 4, 4),\n",
    "                                  generate_code=False)\n",
    "    layer2 = devito.ml.Conv(kernel_size=(2, 2, 2),\n",
    "                            input_size=(1, 2, 3, 3),\n",
    "                            activation=ReLU(),\n",
    "                            generate_code=False)\n",
    "    layer_flat = devito.ml.Flat(input_size=(1, 2, 2, 2),\n",
    "                                generate_code=False)\n",
    "    layer3 = devito.ml.FullyConnectedSoftmax(weight_size=(3, 8),\n",
    "                                             input_size=(8, 1),\n",
    "                                             generate_code=False)\n",
    "    \n",
    "    eqs = layer1.equations() + layer2.equations(layer1.result) + \\\n",
    "            layer_flat.equations(layer2.result) + layer3.equations(layer_flat.result)\n",
    "    \n",
    "    op = Operator(eqs)\n",
    "    \n",
    "    layer2.kernel.data[:] = conv_kernel\n",
    "    layer2.bias.data[:] = conv_bias\n",
    "    \n",
    "    layer3.kernel.data[:] = fc_weights\n",
    "    layer3.bias.data[:] = fc_bias\n",
    "    \n",
    "    layer1.input.data[:] = input_data\n",
    "    \n",
    "    op.apply()\n",
    "    \n",
    "    gradients = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        result = layer3.result.data[i]\n",
    "        if i == expected_results[0]:\n",
    "            result -= 1\n",
    "        gradients.append(result)\n",
    "    \n",
    "    layer3.result_gradients.data[:] = gradients\n",
    "    \n",
    "    dims = [layer3.bias_gradients.dimensions[0],\n",
    "            layer3.kernel_gradients.dimensions[0],\n",
    "            layer3.kernel_gradients.dimensions[1],\n",
    "            layer_flat.result_gradients.dimensions[0],\n",
    "            layer3.kernel.dimensions[1],\n",
    "            layer3.result_gradients.dimensions[0],\n",
    "            layer2.result_gradients.dimensions[0],\n",
    "            layer2.result_gradients.dimensions[1],\n",
    "            layer2.result_gradients.dimensions[2],\n",
    "            layer2.kernel_gradients.dimensions[0],\n",
    "            layer2.kernel_gradients.dimensions[1],\n",
    "            layer2.kernel_gradients.dimensions[2],\n",
    "            layer2.kernel_gradients.dimensions[3],\n",
    "            layer2.bias_gradients.dimensions[0]]\n",
    "    \n",
    "    _, _, layer2_height, layer2_width = layer2.kernel.shape\n",
    "    \n",
    "    backprop_eqs = [\n",
    "        Eq(layer3.bias_gradients[dims[0]], layer3.result_gradients[dims[0]]),\n",
    "        Eq(layer3.kernel_gradients[dims[1], dims[2]],\n",
    "           layer_flat.result[dims[2], 0] * layer3.result_gradients[dims[1]]),\n",
    "        Inc(layer_flat.result_gradients[dims[4]],\n",
    "            layer3.kernel[dims[5], dims[4]] * layer3.result_gradients[dims[5]]),\n",
    "        Eq(layer_flat.result_gradients[dims[3]],\n",
    "           layer_flat.result_gradients[dims[3]] * sign(layer_flat.result[dims[3], 0])),\n",
    "        Eq(layer2.result_gradients[dims[6], dims[7], dims[8]],\n",
    "           layer_flat.result_gradients[dims[6] * layer2_height * layer2_width + dims[7] * layer2_height + dims[8]]),\n",
    "        Inc(layer2.bias_gradients[dims[13]], layer2.result_gradients[dims[13], dims[7], dims[8]]),\n",
    "        Eq(layer2.kernel_gradients[dims[9], dims[10], dims[11], dims[12]],\n",
    "            sum([layer2.result_gradients[dims[9], x, y] * layer1.result[0, dims[10], dims[11] + x, dims[12] + y]\n",
    "                 for x in range(layer2.result_gradients.shape[1])\n",
    "                 for y in range(layer2.result_gradients.shape[2])]))\n",
    "    ]\n",
    "    \n",
    "    backprop_op = Operator(backprop_eqs)\n",
    "    backprop_op.apply()\n",
    "    \n",
    "    return (layer2.kernel_gradients.data, layer2.bias_gradients.data,\n",
    "            layer3.kernel_gradients.data, layer3.bias_gradients.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_kernel = np.random.rand(2, 2, 2)\n",
    "conv_bias = np.random.rand(2)\n",
    "\n",
    "fc_weights = np.random.rand(3, 8)\n",
    "fc_bias = np.random.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array([[[[1, 2, 3, 4],\n",
    "                         [5, 6, 7, 8],\n",
    "                         [9, 10, 11, 12],\n",
    "                         [13, 14, 15, 16]],\n",
    "                        [[-1, -2, 0, 1],\n",
    "                         [-2, -3, 1, 2],\n",
    "                         [3, 4, 2, -1],\n",
    "                         [-2, -3, -4, 9]]]],\n",
    "                     dtype=np.float64)\n",
    "expected = np.array([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksymilian/Desktop/UROP/devito/devito/types/grid.py:206: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  spacing = (np.array(self.extent) / (np.array(self.shape) - 1)).astype(self.dtype)\n",
      "Operator `Kernel` run in 0.01 s\n",
      "Operator `Kernel` run in 0.01 s\n"
     ]
    }
   ],
   "source": [
    "conv_kernel_grad, conv_bias_grad, fc_kernel_grad, fc_bias_grad = backward_pass(conv_kernel, conv_bias, fc_weights, fc_bias, input_data, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are kernel gradients of the convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 3.04044575e-05  2.90224964e-05]\n",
      "   [ 2.48766131e-05  2.34946520e-05]]\n",
      "\n",
      "  [[ 4.13694087e-05  2.12969676e-05]\n",
      "   [-5.52784443e-06  1.93613486e-05]]]\n",
      "\n",
      "\n",
      " [[[-6.66596437e-05 -7.06103813e-05]\n",
      "   [-8.24625944e-05 -8.64133320e-05]]\n",
      "\n",
      "  [[-3.81183194e-05 -1.71252923e-05]\n",
      "   [-1.58029507e-05 -5.76363361e-05]]]]\n"
     ]
    }
   ],
   "source": [
    "print(conv_kernel_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are bias gradients of the convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.38196111e-06 -3.95073767e-06]\n"
     ]
    }
   ],
   "source": [
    "print(conv_bias_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are kernel gradients of the fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.26879304e-04  2.55871633e-04  3.64882635e-04  3.98305128e-04\n",
      "   2.28030656e-04  2.57022985e-04  3.66033987e-04  3.99456480e-04]\n",
      " [ 3.64237093e-06  4.10782024e-06  5.85790716e-06  6.39447932e-06\n",
      "   3.66085499e-06  4.12630430e-06  5.87639122e-06  6.41296338e-06]\n",
      " [-2.30521675e-04 -2.59979454e-04 -3.70740542e-04 -4.04699608e-04\n",
      "  -2.31691511e-04 -2.61149290e-04 -3.71910378e-04 -4.05869443e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(fc_kernel_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are bias gradients of the fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.06513740e-05  1.70999533e-07 -1.08223736e-05]\n"
     ]
    }
   ],
   "source": [
    "print(fc_bias_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with PyTorch\n",
    "To check correctness, we will carry out the same activities using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 2, 2)\n",
    "        self.fc = nn.Linear(8, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(x, 2, stride=(1, 1))\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.double()\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.conv.weight[:] = torch.from_numpy(conv_kernel)\n",
    "    net.conv.bias[:] = torch.from_numpy(conv_bias)\n",
    "    \n",
    "    net.fc.weight[:] = torch.from_numpy(fc_weights)\n",
    "    net.fc.bias[:] = torch.from_numpy(fc_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "input_data_tensor = torch.from_numpy(input_data)\n",
    "outputs = net(input_data_tensor)\n",
    "net.zero_grad()\n",
    "loss = criterion(outputs, torch.from_numpy(expected))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the relative convolutional layer kernel error along with the maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.93897533e-13 2.25077747e-13]\n",
      "   [3.40493678e-13 3.88209657e-13]]\n",
      "\n",
      "  [[4.19325180e-14 8.24085523e-14]\n",
      "   [4.66432861e-13 1.67294854e-13]]]\n",
      "\n",
      "\n",
      " [[[7.23781197e-14 7.69655012e-14]\n",
      "   [8.84189937e-14 9.17477454e-14]]\n",
      "\n",
      "  [[2.23989206e-14 7.32021818e-14]\n",
      "   [1.56939835e-13 4.18546701e-14]]]]\n",
      "4.664328608794458e-13\n"
     ]
    }
   ],
   "source": [
    "pytorch_conv_kernel_grad = net.conv.weight.grad.numpy()\n",
    "conv_kernel_error = abs(conv_kernel_grad - pytorch_conv_kernel_grad) / abs(pytorch_conv_kernel_grad)\n",
    "print(conv_kernel_error)\n",
    "print(np.amax(conv_kernel_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the relative convolutional layer bias error along with the maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.65819940e-13 1.56939835e-13]\n",
      "4.658199399058854e-13\n"
     ]
    }
   ],
   "source": [
    "pytorch_conv_bias_grad = net.conv.bias.grad.numpy()\n",
    "conv_bias_error = abs(conv_bias_grad - pytorch_conv_bias_grad) / abs(pytorch_conv_bias_grad)\n",
    "print(conv_bias_error)\n",
    "print(np.amax(conv_bias_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the relative fully connected layer kernel error along with the maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.78362880e-14 2.79661104e-14 2.79309001e-14 2.79009018e-14\n",
      "  2.78146054e-14 2.78408343e-14 2.78430441e-14 2.78204831e-14]\n",
      " [2.79059864e-14 2.80432137e-14 2.77625304e-14 2.78172639e-14\n",
      "  2.77650860e-14 2.77123158e-14 2.79634874e-14 2.78691677e-14]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "2.8043213667842844e-14\n"
     ]
    }
   ],
   "source": [
    "pytorch_fc_kernel_grad = net.fc.weight.grad.numpy()\n",
    "fc_kernel_error = abs(fc_kernel_grad - pytorch_fc_kernel_grad) / abs(pytorch_fc_kernel_grad)\n",
    "print(fc_kernel_error)\n",
    "print(np.amax(fc_kernel_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the relative fully connected layer bias error along with the maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.78331726e-14 2.78630020e-14 0.00000000e+00]\n",
      "2.7863001967068124e-14\n"
     ]
    }
   ],
   "source": [
    "pytorch_fc_bias_grad = net.fc.bias.grad.numpy()\n",
    "fc_bias_error = abs(fc_bias_grad - pytorch_fc_bias_grad) / abs(pytorch_fc_bias_grad)\n",
    "print(fc_bias_error)\n",
    "print(np.amax(fc_bias_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the maximum overall error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.664328608794458e-13\n"
     ]
    }
   ],
   "source": [
    "print(max(np.amax(conv_kernel_error), np.amax(conv_bias_error), np.amax(fc_kernel_error), np.amax(fc_bias_error)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
